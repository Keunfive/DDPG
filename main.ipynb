{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_storage/DDQN_spline_1_1_episode_50\\assets\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'graph_plot' object has no attribute 'Episode_Reward_plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mreplay_memory/replay_memory_\u001b[39m\u001b[39m{\u001b[39;00mepisode\u001b[39m}\u001b[39;00m\u001b[39m.p\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fr:    \n\u001b[0;32m    105\u001b[0m             pickle\u001b[39m.\u001b[39mdump(replay_memory, fr)\n\u001b[1;32m--> 107\u001b[0m         mp\u001b[39m.\u001b[39;49mInference\u001b[39m.\u001b[39;49mgraph\u001b[39m.\u001b[39;49mgraph_plot()\u001b[39m.\u001b[39;49mEpisode_Reward_plot(reward_list, episode)\n\u001b[0;32m    109\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39m'\u001b[39m\u001b[39mFinish at: \u001b[39m\u001b[39m'\u001b[39m,\u001b[39mstr\u001b[39m(datetime\u001b[39m.\u001b[39mtimedelta(seconds\u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start))))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'graph_plot' object has no attribute 'Episode_Reward_plot'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import Meshpkg as mp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\"Parameter 정의\"\n",
    "p = mp.params\n",
    "\n",
    "\"Seed 설정\"\n",
    "seed = 42\n",
    "mp.Initialize.my_seed.my_seed_everywhere(42)\n",
    "\n",
    "\"Episode 수\" \n",
    "n_episodes = 1000\n",
    "\n",
    "\"model, target model(Double DQN) 정의\"\n",
    "model = mp.Initialize.model_definition.NNmodel().dense_multi()\n",
    "\n",
    "model_target = keras.models.clone_model(model)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\"Replay_memory 정의\"\n",
    "replay_memory = deque(maxlen = p.buffer_size)\n",
    "\n",
    "\"Inference 주기\"\n",
    "episode_inference = 5\n",
    "\n",
    "\"Neural Network model 저장 주기, 저장 여부\"\n",
    "episode_save = 50\n",
    "save_model = True\n",
    "\n",
    "\"Episode - reward list/ Time initialize\"\n",
    "reward_list = [ ]\n",
    "reward_inf_list = [ ]\n",
    "start = time.time()\n",
    "\n",
    "for episode in range(1, n_episodes+1): \n",
    "    \n",
    "    s = mp.Env.Step.step_class()\n",
    "    state = s.reset()\n",
    "    step_ended = 0\n",
    "    # step_bar = tqdm(range(1, p.num_layer+1), desc = f'< Episode: {episode} > Steps' , leave = True, maxinterval = 0.1, position = 1)\n",
    "    reward_episode = 0\n",
    "    epsilon = max(((p.epsilon_start)**episode), p.epsilon_min) # epsilon 0.01 도달 까지 4603 필요\n",
    "    for step in range(1, p.num_layer+1):\n",
    "        _, actions = mp.Env.Action.get_action(model, s.volume_mesh, epsilon)\n",
    "        next_state, reward, done, info, steps =  s.step_func(actions, step, episode)\n",
    "        replay_memory.append((state, actions, reward, next_state, done, steps))\n",
    "        state = next_state\n",
    "        reward_episode += np.average(reward)\n",
    "        if any(done) == 1:\n",
    "            step_ended = step\n",
    "            reward_list.append(reward_episode)\n",
    "            \n",
    "            with open(\"episode_step_record.txt\", 'a') as epistep_file:\n",
    "                epistep_file.write(f' \\n<episode: {episode}> Step ended: {step_ended} ')\n",
    "                if episode == 1:\n",
    "                    end1 = start\n",
    "                end2 = time.time()\n",
    "                epi_time = str(datetime.timedelta(seconds= (end2 - end1)))\n",
    "                short1 = epi_time.split(\".\")[0]\n",
    "                total_time = str(datetime.timedelta(seconds= (end2 - start)))\n",
    "                short2 = total_time.split(\".\")[0]\n",
    "                epistep_file.write(f\"  Time per episode: {short1} (Total: {short2})\\n\") # epi 시간, 누적시간 출력\n",
    "                end1 = end2\n",
    "                \n",
    "            if step_ended != p.num_layer:\n",
    "                replay_memory = mp.Train.replay_penalty.penalty_reward(replay_memory, info, step_ended, 3, 3)\n",
    "            break\n",
    "    # step_bar.close()\n",
    "    \"replay memory 다 차면, episode 끝나고 model training 시작\"\n",
    "    if len(replay_memory) == p.buffer_size:\n",
    "        loss_mean, state_new, next_state_new, Q_values, target_Q_values = mp.Train.model_training.training_step_mean_DDQN(model, model_target, replay_memory)\n",
    "        \n",
    "    \"episode (episode_inference)회마다 Inference\"\n",
    "    if episode % (episode_inference) == 0:\n",
    "        volume_mesh_inf, reward_inf_mean = mp.Inference.inference.inference_step(model, episode)\n",
    "        mp.Inference.render.render(volume_mesh_inf, episode)\n",
    "        reward_inf_list.append(reward_inf_mean)\n",
    "\n",
    "    \"episode (episode_target)회마다 Target model update\"\n",
    "    if episode % (p.episode_target) == 0:\n",
    "        model_target.set_weights(model.get_weights())\n",
    "\n",
    "    \"episode (episode_save)회마다 model, replay memory, episode-reward 저장\"\n",
    "    if (episode % (episode_save) == 0) and (save_model):\n",
    "        model.save(f'model_storage/DDQN_{p.mesh_name}_episode_{episode}')\n",
    "        \n",
    "        mp.Inference.graph.graph_plot().createFolder('replay_memory')\n",
    "        with open(f'replay_memory/replay_memory_{episode}.p', 'wb') as fr:    \n",
    "            pickle.dump(replay_memory, fr)\n",
    "            \n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_plot(reward_list, episode)\n",
    "\n",
    "print ('Finish at: ',str(datetime.timedelta(seconds= (time.time() - start))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import Meshpkg as mp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "\"Parameter 정의\"\n",
    "p = mp.params\n",
    "\n",
    "\"Seed 설정\"\n",
    "seed = 42\n",
    "mp.Initialize.my_seed.my_seed_everywhere(42)\n",
    "\n",
    "\"Episode 수\"\n",
    "start_episode = 4000\n",
    "n_episodes = 5000\n",
    "\n",
    "\"model, target model(Double DQN) 정의\"\n",
    "\n",
    "model = tf.keras.models.load_model(f'model_storage/DDQN_spline_1_episode_{start_episode}')\n",
    "\n",
    "model_target = keras.models.clone_model(model)\n",
    "model_target.set_weights(model.get_weights())\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\"Replay_memory 정의\"\n",
    "with open(f'replay_memory/replay_memory_{start_episode}.p', 'rb') as fr:  \n",
    "    replay_memory = pickle.load(fr)\n",
    "\n",
    "\"Inference 주기\"\n",
    "episode_inference = 5\n",
    "\n",
    "\"Neural Network model 저장 주기, 저장 여부\"\n",
    "episode_save = 50\n",
    "save_model = True\n",
    "\n",
    "\"Episode - reward list/ Time initialize\"\n",
    "with open(f'Episode_reward_train/reward_epi_{start_episode}.p', 'rb') as fe:     \n",
    "    reward_list = pickle.load(fe)\n",
    "##\n",
    "with open(f'Episode_reward_inf/reward_epi_{start_episode}.p', 'rb') as fei:     \n",
    "    reward_list = pickle.load(fei)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for episode in range(start_episode+1, n_episodes+1): \n",
    "    \n",
    "    s = mp.Env.Step.step_class()\n",
    "    state = s.reset()\n",
    "    step_ended = 0\n",
    "    # step_bar = tqdm(range(1, p.num_layer+1), desc = f'< Episode: {episode} > Steps' , leave = True, maxinterval = 0.1, position = 1)\n",
    "    reward_episode = 0\n",
    "    epsilon = max(((p.epsilon_start)**episode), p.epsilon_min) # epsilon 0.01 도달 까지 4603 필요\n",
    "    for step in range(1, p.num_layer+1):\n",
    "        _, actions = mp.Env.Action.get_action(model, s.volume_mesh, epsilon)\n",
    "        next_state, reward, done, info, steps =  s.step_func(actions, step, episode)\n",
    "        replay_memory.append((state, actions, reward, next_state, done, steps))\n",
    "        state = next_state\n",
    "        reward_episode += np.average(reward)\n",
    "        if any(done) == 1:\n",
    "            step_ended = step\n",
    "            reward_list.append(reward_episode)\n",
    "            \n",
    "            with open(\"Episode_Step_record.txt\", 'a') as epistep_file:\n",
    "                epistep_file.write(f' \\n<episode: {episode}> Step ended: {step_ended} ')\n",
    "                if episode == start_episode+1:\n",
    "                    end1 = start\n",
    "                end2 = time.time()\n",
    "                epi_time = str(datetime.timedelta(seconds= (end2 - end1)))\n",
    "                short1 = epi_time.split(\".\")[0]\n",
    "                total_time = str(datetime.timedelta(seconds= (end2 - start)))\n",
    "                short2 = total_time.split(\".\")[0]\n",
    "                epistep_file.write(f\"  Time per episode: {short1} (Total: {short2})\\n\") # epi 시간, 누적시간 출력\n",
    "                end1 = end2\n",
    "                \n",
    "            if step_ended != p.num_layer:\n",
    "                replay_memory = mp.Train.replay_penalty.penalty_reward(replay_memory, info, step_ended, 3, 3)\n",
    "            break\n",
    "    # step_bar.close()\n",
    "    \"replay memory 다 차면, episode 끝나고 model training 시작\"\n",
    "    if len(replay_memory) == p.buffer_size:\n",
    "        loss_mean, state_new, next_state_new, Q_values, target_Q_values = mp.Train.model_training.training_step_mean_DDQN(model, model_target, replay_memory)\n",
    "        \n",
    "    \"episode (episode_inference)회마다 Inference\"\n",
    "    if episode % (episode_inference) == 0:\n",
    "        volume_mesh_inf, reward_inf_mean = mp.Inference.inference.inference_step(model, episode)\n",
    "        mp.Inference.render.render(volume_mesh_inf, episode)\n",
    "        reward_inf_list.append(reward_inf_mean)\n",
    "    \"episode (episode_target)회마다 Target model update\"\n",
    "    if episode % (p.episode_target) == 0:\n",
    "        # model_target.set_weights(model.get_weights())\n",
    "        \n",
    "        mp.Train.target_update.update_target.soft_update(model_target.variables, model.variables, p.tau)\n",
    "    \"episode (episode_save)회마다 model, replay memory, episode-reward 저장\"\n",
    "    if (episode % (episode_save) == 0) and (save_model):\n",
    "        model.save(f'model_storage/DDQN_{p.mesh_name}_episode_{episode}')\n",
    "        \n",
    "        mp.Inference.graph.graph_plot().createFolder('replay_memory')\n",
    "        with open(f'replay_memory/replay_memory_{episode}.p', 'wb') as fr:    \n",
    "            pickle.dump(replay_memory, fr)\n",
    "            \n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_train_plot(reward_list, episode)\n",
    "        mp.Inference.graph.graph_plot().Episode_Reward_inf_plot(reward_inf_list, episode)\n",
    "\n",
    "\n",
    "print ('Finish at: ',str(datetime.timedelta(seconds= (time.time() - start))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
